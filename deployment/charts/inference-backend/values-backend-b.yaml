replicaCount: 1

image:
  vllm:
    repository: vllm/vllm-openai
    tag: latest
    pullPolicy: IfNotPresent

status:
  enabled: false

nodeSelector:
  workload: gpu

tolerations:
  - key: "nvidia.com/gpu"
    operator: "Equal"
    value: "present"
    effect: "NoSchedule"

affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
            - key: app.kubernetes.io/instance
              operator: In
              values:
                - inference-backend-a
        topologyKey: "kubernetes.io/hostname"

backend:
  id: "backend-b"
  vllmArgs:
    - "--model=hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4"
    - "--enforce-eager"
    - "--host=0.0.0.0"
    - "--port=8000"
    - "--gpu-memory-utilization=0.80"
    - "--max-model-len=2048"
    - "--max-num-batched-tokens=1024"
    - "--max-num-seqs=4"
    - "--attention-backend=TRITON_ATTN" 
  env: []
  extraVolumes:
    - name: hf-cache
      hostPath:
        path: /var/lib/hf-cache
        type: DirectoryOrCreate
    - name: dshm
      emptyDir:
        medium: Memory
        sizeLimit: 1Gi
  extraVolumeMounts:
    - name: hf-cache
      mountPath: /cache/huggingface
    - name: dshm
      mountPath: /dev/shm

resources:
  vllm:
    requests:
      cpu: "2"
      memory: "8Gi"
    limits:
      cpu: "4"
      memory: "14Gi"
      nvidia.com/gpu: 1

warmup:
  enabled: false
  jobEnabled: true
  baseUrl: "http://inference-backend-backend-b.inference-backend.svc.cluster.local:8000"
  modelId: "TheBloke/Llama-3.2-8B-Instruct-AWQ"
  waitLoops: 180
  sleepSeconds: 5
