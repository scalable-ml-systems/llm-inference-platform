backend:
  id: "vllm-b"
  model:
    id: "llama-3.2-3b-instruct"
  vllmArgs:
    - "--model=meta-llama/Llama-3.2-3B-Instruct"
    - "--max-model-len=4096"
    - "--gpu-memory-utilization=0.85"
    - "--dtype=float16"

nodeSelector:
  workload: gpu

tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"
